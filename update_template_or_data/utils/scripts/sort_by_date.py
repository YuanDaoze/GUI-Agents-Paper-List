# Parsing and ordering all paper entries based on the extracted file content

import pandas as pd
import calendar
import re


# è¯»å–æ–‡ä»¶å†…å®¹
paper_source="update_template_or_data/update_paper_list.md"
with open(paper_source, "r", encoding="utf-8") as file:
    sample_input = file.read()


def parse_date_with_defaults(date_str):
    try:
        return pd.to_datetime(date_str, errors='coerce')
    except ValueError:
        try:
            parsed_date = parser.parse(date_str, fuzzy=True, default=pd.Timestamp.now())
            last_day = calendar.monthrange(parsed_date.year, parsed_date.month)[1]
            return pd.Timestamp(year=parsed_date.year, month=parsed_date.month, day=last_day)
        except ValueError:
            try:
                year = int(re.search(r"\b(20\d{2})\b", date_str).group(0))
                return pd.Timestamp(year=year, month=12, day=31)
            except (ValueError, AttributeError):
                return pd.NaT


# Define regex pattern based on new input format
new_format_pattern = re.compile(
    r"- \[(.*?)\]\((.*?)\)\s+"
    r"- (.*?)\s+"
    r"- ğŸ›ï¸ Institutions: (.*?)\s+"
    r"- ğŸ“… Date: (.*?)\s+"
    r"- ğŸ“‘ Publisher: (.*?)\s+"
    r"- ğŸ’» Env: \[(.*?)\]\s+"
    r"- ğŸ”‘ Key: (.*?)\s+"
    r"- ğŸ“– TLDR: (.*?)\n",
    re.DOTALL
)

# Extract data and process it
parsed_entries = []
for match in new_format_pattern.findall(sample_input):
    title, link, authors, institutions, date, publisher, env, keywords, tldr = match
    parsed_date = parse_date_with_defaults(date)
    # Wrap keywords in brackets, split by commas
    formatted_keywords = ", ".join([f"{kw.strip()}" for kw in keywords.split(",")])
    parsed_entries.append((title, link, authors, institutions, date, parsed_date, publisher, f"[{env.strip()}]",
                           formatted_keywords, tldr))

# Convert to DataFrame and sort
papers_df = pd.DataFrame(parsed_entries, columns=[
    'Title', 'Link', 'Authors', 'Institutions', 'Original Date', 'Parsed Date', 'Publisher', 'Env', 'Keywords', 'TLDR'
]).drop_duplicates(subset='Title', keep='first')
papers_df.sort_values(by='Parsed Date', ascending=False, inplace=True)

# Format output to new specified Markdown structure
sorted_markdown = []
for _, row in papers_df.iterrows():
    markdown_entry = f"- [{row['Title']}]({row['Link']})\n" \
                     f"    - {row['Authors']}\n" \
                     f"    - ğŸ›ï¸ Institutions: {row['Institutions']}\n" \
                     f"    - ğŸ“… Date: {row['Original Date']}\n" \
                     f"    - ğŸ“‘ Publisher: {row['Publisher']}\n" \
                     f"    - ğŸ’» Env: {row['Env']}\n" \
                     f"    - ğŸ”‘ Key: {row['Keywords']}\n" \
                     f"    - ğŸ“– TLDR: {row['TLDR']}\n"
    sorted_markdown.append(markdown_entry)

# Combine into final output
final_output = "\n".join(sorted_markdown)

# print((final_output))

print(papers_df)

with open("update_template_or_data/update_paper_list.md", "w", encoding="utf-8") as file:
    file.write(final_output)

import os

# 1. åˆ›å»ºå­é›†çš„æ–‡ä»¶å¤¹ï¼Œå¦‚æœæ²¡æœ‰çš„è¯
subgroup_dir = "grouped_by_env"
if not os.path.exists(subgroup_dir):
    os.makedirs(subgroup_dir)

# 2. å®šä¹‰ä¸€ä¸ªæ˜ å°„è¡¨ï¼Œç”¨äºä¸åŒçš„envå…³é”®å­—
env_keywords = {
    "web": "env_web.md",
    "desktop": "env_desktop.md",
    "mobile": "env_mobile.md",
    "gui": "env_gui.md",
    "general": "env_general.md"
}

# 3. æ ¹æ®envå­—æ®µè¿›è¡Œè¿‡æ»¤å¹¶ç”Ÿæˆç›¸åº”çš„Markdownæ–‡ä»¶
for env_key, file_name in env_keywords.items():
    filtered_df = papers_df[papers_df['Env'].str.contains(env_key, case=False, na=False)]

    # 4. ç”Ÿæˆæ¯ä¸ªenvå­é›†åˆçš„Markdown
    if not filtered_df.empty:
        sorted_markdown = []
        for _, row in filtered_df.iterrows():
            markdown_entry = f"- [{row['Title']}]({row['Link']})\n" \
                             f"    - {row['Authors']}\n" \
                             f"    - ğŸ›ï¸ Institutions: {row['Institutions']}\n" \
                             f"    - ğŸ“… Date: {row['Original Date']}\n" \
                             f"    - ğŸ“‘ Publisher: {row['Publisher']}\n" \
                             f"    - ğŸ’» Env: {row['Env']}\n" \
                             f"    - ğŸ”‘ Key: {row['Keywords']}\n" \
                             f"    - ğŸ“– TLDR: {row['TLDR']}\n"
            sorted_markdown.append(markdown_entry)

        # 5. å†™å…¥æ–‡ä»¶
        final_output = "\n".join(sorted_markdown)
        file_path = os.path.join(subgroup_dir, file_name)
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(final_output)
        print(f"ç”Ÿæˆæ–‡ä»¶ï¼š{file_path}")
from collections import Counter
import os

# 1. åˆå§‹åŒ–ä¸€ä¸ªCounteræ¥ç»Ÿè®¡ä½œè€…ä½œå“æ•°é‡
author_counter = Counter()

# 2. éå†è®ºæ–‡æ¡ç›®ï¼Œç»Ÿè®¡æ¯ä¸ªä½œè€…çš„ä½œå“æ•°é‡
for _, row in papers_df.iterrows():
    authors = row['Authors']
    author_list = [author.strip() for author in authors.split(',')]  # å‡è®¾ä½œè€…ä»¥é€—å·åˆ†éš”
    author_counter.update(author_list)

# 3. è·å–ä½œå“æœ€å¤šçš„å‰15ä¸ªä½œè€…
top_15_authors = [author for author, _ in author_counter.most_common(15)]

# 4. ä¸ºæ¯ä¸ªä½œè€…ç”Ÿæˆä¸€ä¸ªæ–‡ä»¶ï¼ŒåªåŒ…å«è¯¥ä½œè€…çš„è®ºæ–‡
subgroup_dir = "grouped_by_authors"
if not os.path.exists(subgroup_dir):
    os.makedirs(subgroup_dir)

for author in top_15_authors:
    # ç­›é€‰å‡ºåŒ…å«è¯¥ä½œè€…çš„è®ºæ–‡
    author_papers_df = papers_df[papers_df['Authors'].str.contains(author, case=False, na=False)]

    # ç”Ÿæˆæ¯ä¸ªä½œè€…çš„Markdownå†…å®¹
    sorted_markdown = []
    for _, row in author_papers_df.iterrows():
        markdown_entry = f"- [{row['Title']}]({row['Link']})\n" \
                         f"    - {row['Authors']}\n" \
                         f"    - ğŸ›ï¸ Institutions: {row['Institutions']}\n" \
                         f"    - ğŸ“… Date: {row['Original Date']}\n" \
                         f"    - ğŸ“‘ Publisher: {row['Publisher']}\n" \
                         f"    - ğŸ’» Env: {row['Env']}\n" \
                         f"    - ğŸ”‘ Key: {row['Keywords']}\n" \
                         f"    - ğŸ“– TLDR: {row['TLDR']}\n"
        sorted_markdown.append(markdown_entry)

    # 5. ä¿å­˜æ–‡ä»¶
    author_filename = f"author_{author.replace(' ', '_')}.md"
    author_file_path = os.path.join(subgroup_dir, author_filename)
    with open(author_file_path, "w", encoding="utf-8") as file:
        file.write(f"# {author}'s Papers\n\n")
        file.write("\n".join(sorted_markdown))

    print(f"ç”Ÿæˆæ–‡ä»¶ï¼š{author_file_path}")


# 
# import matplotlib.pyplot as plt
# import seaborn as sns
# from collections import Counter
# #
# # 1. åˆå§‹åŒ–ä¸€ä¸ªCounteræ¥ç»Ÿè®¡ä½œè€…ä½œå“æ•°é‡
# author_counter = Counter()
#
# # 2. éå†è®ºæ–‡æ¡ç›®ï¼Œç»Ÿè®¡æ¯ä¸ªä½œè€…çš„ä½œå“æ•°é‡
# for _, row in papers_df.iterrows():
#     authors = row['Authors']
#     author_list = [author.strip() for author in authors.split(',')]  # å‡è®¾ä½œè€…ä»¥é€—å·åˆ†éš”
#     author_counter.update(author_list)
#
# # 3. è·å–ä½œå“æœ€å¤šçš„å‰15ä¸ªä½œè€…
# top_15_authors = [author for author, _ in author_counter.most_common(15)]
#
# # 4. è·å–å‰15ä¸ªä½œè€…çš„ä½œå“æ•°é‡
# top_15_counts = [author_counter[author] for author in top_15_authors]
#
# # 5. åˆ›å»ºæ¡å½¢å›¾
# plt.figure(figsize=(10, 6))  # è®¾ç½®å›¾è¡¨å¤§å°
# sns.barplot(x=top_15_counts, y=top_15_authors, palette="viridis")
#
# # 6. è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œæ ‡ç­¾
# plt.title("Top 15 Authors by Number of Papers", fontsize=16)
# plt.xlabel("Number of Papers", fontsize=14)
# plt.ylabel("Authors", fontsize=14)
#
# # 7. ç¾åŒ–å›¾è¡¨ï¼ˆå¯é€‰ï¼‰
# plt.xticks(fontsize=12)
# plt.yticks(fontsize=12)
# plt.tight_layout()
#
# # 8. ä¿å­˜å›¾è¡¨åˆ°æ–‡ä»¶
# plt.savefig("update_template_or_data/top_15_authors.png")
#
# # 9. æ˜¾ç¤ºå›¾è¡¨
# plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter
import re
def remove_square_brackets(s):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤å¼€å¤´å’Œç»“å°¾çš„æ–¹æ‹¬å·
    return re.sub(r'^\[|\]$', '', s)
# 1. å®šä¹‰æ’é™¤çš„å…³é”®è¯
# excluded_keywords = {"[model]", "[framework]", "[benchmark]", "[dataset]"}


from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

# 1. å®šä¹‰æ’é™¤çš„å…³é”®è¯
excluded_keywords = {"model", "framework", "benchmark", "dataset"}
excluded_keywords = {}

# 2. æå–æ‰€æœ‰è®ºæ–‡çš„å…³é”®è¯å¹¶è®¡ç®—é¢‘ç‡ï¼ˆæ’é™¤ç‰¹å®šå…³é”®è¯ï¼‰
all_keywords = []
for _, row in papers_df.iterrows():
    keywords = row['Keywords']
    # æå–å¹¶æ’é™¤ä¸éœ€è¦çš„å…³é”®è¯
    filtered_keywords = [kw.strip() for kw in keywords.split(", ") if kw.strip().lower() not in excluded_keywords]
    all_keywords.extend(filtered_keywords)

# # 3. è®¡ç®—æ¯ä¸ªå…³é”®è¯çš„é¢‘ç‡
# keyword_counts = Counter(all_keywords)
#
# # 4. åˆ›å»ºè¯äº‘å›¾ï¼Œè°ƒæ•´åˆ†è¾¨ç‡å’Œå­—ä½“å¤§å°
# wordcloud = WordCloud(
#     width=1200,  # å¢å¤§å›¾åƒå®½åº¦
#     height=500,  # å¢å¤§å›¾åƒé«˜åº¦
#     background_color="white",  # èƒŒæ™¯é¢œè‰²
#     max_words=200,  # æ˜¾ç¤ºçš„æœ€å¤šè¯æ•°
#     colormap="viridis",  # è¯äº‘çš„é¢œè‰²æ˜ å°„
#     contour_width=0,  # è¯äº‘è¾¹æ¡†å®½åº¦ï¼ˆ0è¡¨ç¤ºæ²¡æœ‰è¾¹æ¡†ï¼‰
#     contour_color='black',  # è¯äº‘è¾¹æ¡†é¢œè‰²
#     max_font_size=50,  # æœ€å¤§å­—ä½“å¤§å°
#     min_font_size=10,  # æœ€å°å­—ä½“å¤§å°
# ).generate_from_frequencies(keyword_counts)
#
# # 5. æ˜¾ç¤ºå¹¶ä¿å­˜è¯äº‘å›¾ï¼Œå¢åŠ DPIæ¥æé«˜æ¸…æ™°åº¦
# plt.figure(figsize=(12, 6))  # è®¾ç½®å›¾åƒå°ºå¯¸
# plt.imshow(wordcloud, interpolation='bilinear')  # æ’å€¼æ˜¾ç¤ºè¯äº‘å›¾
# plt.axis('off')  # ä¸æ˜¾ç¤ºåæ ‡è½´
# plt.tight_layout(pad=0)
#
# # 6. ä¿å­˜å›¾ç‰‡æ–‡ä»¶ï¼Œå¹¶è®¾ç½®é«˜DPI
# wordcloud_img_path = "update_template_or_data/keyword_wordcloud.png"
# plt.savefig(wordcloud_img_path, format='png', dpi=460)  # å¢åŠ DPIï¼Œç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒ
# plt.close()  # å…³é—­å›¾åƒï¼Œé‡Šæ”¾èµ„æº
#
# print(f"å…³é”®è¯è¯äº‘å›¾å·²ä¿å­˜ä¸º: {wordcloud_img_path}")
